{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fNIRS-Based Glucose Prediction Analysis\n",
    "\n",
    "This notebook contains the data, Python scripts, and machine learning models for a project aimed at predicting blood glucose levels from functional near-infrared spectroscopy (fNIRS) data. The project involves data preprocessing, feature engineering, model training, and robust evaluation across different sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, we'll install the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn joblib xgboost lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Reconstruction\n",
    "\n",
    "Next, we'll reconstruct the fNIRS data files from their split parts. This is necessary because the full data files are too large to be stored in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python reconstruct_file.py first_fnirs_log_part1_of_2.csv\n",
    "!python reconstruct_file.py second_fnirs_log_part1_of_2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main Analysis Script\n",
    "\n",
    "Now we'll run the main analysis script. This script will perform the following steps:\n",
    "\n",
    "1.  **Load and Preprocess Data:** Load the reconstructed fNIRS and CGM data, and perform preprocessing and feature engineering.\n",
    "2.  **Configure Experiments:** Define the models, hyperparameter grids, and cross-validation strategies to be used.\n",
    "3.  **Run Experiments:** Run the generalization and combined holdout experiments.\n",
    "4.  **Evaluate Models:** Evaluate the performance of each model using RMSE, MAE, R², and Clarke Error Grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, TimeSeriesSplit, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Master Configuration ---\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Path and Data Configuration ---\n",
    "BASE_DIR = os.getcwd()\n",
    "S1_FNIRS_PATH = os.path.join(BASE_DIR, 'first_fnirs_log_reconstructed.csv')\n",
    "S1_CGM_PATH = os.path.join(BASE_DIR, 'first_cgm_log.csv')\n",
    "S2_FNIRS_PATH = os.path.join(BASE_DIR, 'second_fnirs_log_reconstructed.csv')\n",
    "S2_CGM_PATH = os.path.join(BASE_DIR, 'second_cgm_log.csv')\n",
    "S1_CGM_COLUMN = 'Scan Glucose (mmol/L)'\n",
    "S2_CGM_COLUMN = 'Scan Glucose (mmol/L)'\n",
    "\n",
    "# --- Preprocessing and Feature Engineering Constants ---\n",
    "SMOOTHING_WINDOW = 30\n",
    "EPOCH_DURATION_S = 60\n",
    "EPOCH_OVERLAP_RATIO = 0.5\n",
    "N_FEATURES_TO_SELECT = 40\n",
    "USABLE_CHANNELS = [\n",
    "    (2,5,'short'),(3,6,'short'),(6,12,'short'),(7,13,'short'),(1,2,'long'),(1,6,'long'),(1,9,'long'),\n",
    "    (2,6,'long'),(2,7,'long'),(3,5,'long'),(3,7,'long'),(4,2,'long'),(4,5,'long'),(4,6,'long'),\n",
    "    (4,7,'long'),(5,6,'long'),(5,9,'long'),(5,12,'long'),(6,3,'long'),(6,5,'long'),(6,13,'long'),\n",
    "    (7,3,'long'),(7,9,'long'),(7,12,'long'),(8,2,'long'),(8,3,'long'),(8,11,'long'),(8,13,'long'),\n",
    "]\n",
    "DPF_WL1=6.25; DPF_WL2=4.89; D_SHORT_CM=0.8; D_LONG_CM=3.0; LN10=np.log(10)\n",
    "EXT_MOLAR_HBO_WL1=803.1/LN10; EXT_MOLAR_HHB_WL1=2278.1/LN10; EXT_MOLAR_HBO_WL2=1058.0/LN10; EXT_MOLAR_HHB_WL2=740.0/LN10\n",
    "EPS_HBO_WL1_uM=EXT_MOLAR_HBO_WL1/1.0e6; EPS_HHB_WL1_uM=EXT_MOLAR_HHB_WL1/1.0e6; EPS_HBO_WL2_uM=EXT_MOLAR_HBO_WL2/1.0e6; EPS_HHB_WL2_uM=EXT_MOLAR_HHB_WL2/1.0e6\n",
    "E_MATRIX_uM=np.array([[EPS_HBO_WL1_uM,EPS_HHB_WL1_uM],[EPS_HBO_WL2_uM,EPS_HHB_WL2_uM]])\n",
    "try: E_INV_MATRIX_uM=np.linalg.inv(E_MATRIX_uM)\n",
    "except np.linalg.LinAlgError: print(\"FATAL ERROR: Extinction coefficient matrix is singular.\"); exit()\n",
    "\n",
    "\n",
    "# --- Model and Hyperparameter Tuning Configuration ---\n",
    "MODELS_AND_PARAMS = {\n",
    "    'Ridge': {\n",
    "        'model': Ridge(),\n",
    "        'params': {\n",
    "            'regressor__alpha': [0.1, 1.0, 10, 100]\n",
    "        }\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': [50, 100, 200],\n",
    "            'regressor__max_depth': [None, 10, 20],\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': xgb.XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': [100, 200],\n",
    "            'regressor__learning_rate': [0.01, 0.1],\n",
    "            'regressor__max_depth': [3, 5, 7],\n",
    "        }\n",
    "    },\n",
    "    'SVR': {\n",
    "        'model': SVR(),\n",
    "        'params': {\n",
    "            'regressor__C': [0.1, 1, 10],\n",
    "            'regressor__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'model': ElasticNet(random_state=42),\n",
    "        'params': {\n",
    "            'regressor__alpha': [0.1, 1.0, 10],\n",
    "            'regressor__l1_ratio': [0.1, 0.5, 0.9]\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': lgb.LGBMRegressor(random_state=42, n_jobs=-1),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': [100, 200],\n",
    "            'regressor__learning_rate': [0.01, 0.1],\n",
    "            'regressor__num_leaves': [31, 50]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Experiment Configuration ---\n",
    "CV_STRATEGY = 'TimeSeriesSplit'  # Options: 'TimeSeriesSplit', 'BlockedKFold', 'KFold'\n",
    "N_CV_SPLITS = 5\n",
    "CV_GAP_EPOCHS = 2 # Only used for BlockedKFold\n",
    "RUN_GENERALIZATION_EXPERIMENTS = True\n",
    "RUN_COMBINED_HOLDOUT_EXPERIMENT = True\n",
    "TRAIN_SPLIT_RATIO = 0.7 # For combined holdout experiment\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Helper Classes and Functions ---\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "class BlockedKFold():\n",
    "    def __init__(self, n_splits=5, gap=0): self.n_splits, self.gap = n_splits, gap\n",
    "    def get_n_splits(self, X=None, y=None, groups=None): return self.n_splits\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n = len(X)\n",
    "        k_size = n // self.n_splits\n",
    "        indices = np.arange(n)\n",
    "        for i in range(self.n_splits):\n",
    "            start = i * k_size\n",
    "            end = start + k_size\n",
    "            test_indices = indices[start:end]\n",
    "            train_indices = np.concatenate([indices[:max(0, start - self.gap)], indices[end + self.gap:]])\n",
    "            yield train_indices, test_indices\n",
    "\n",
    "def get_cv_strategy(strategy_name, n_splits, gap):\n",
    "    if strategy_name == 'TimeSeriesSplit':\n",
    "        return TimeSeriesSplit(n_splits=n_splits)\n",
    "    elif strategy_name == 'BlockedKFold':\n",
    "        return BlockedKFold(n_splits=n_splits, gap=gap)\n",
    "    else: # Default to KFold\n",
    "        return KFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "def plot_clarke_error_grid(y_true_mmol, y_pred_mmol, title, plots_folder):\n",
    "    \"\"\"Generates and saves a Clarke Error Grid plot.\"\"\"\n",
    "    y_true = np.array(y_true_mmol) * 18.0182\n",
    "    y_pred = np.array(y_pred_mmol) * 18.0182\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.scatter(y_true, y_pred, c='k', s=25, zorder=2)\n",
    "    ax.set_xlabel(\"Reference Glucose (mg/dL)\", fontsize=14)\n",
    "    ax.set_ylabel(\"Predicted Glucose (mg/dL)\", fontsize=14)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xticks(range(0, 401, 50))\n",
    "    ax.set_yticks(range(0, 401, 50))\n",
    "    ax.set_xlim(0, 400)\n",
    "    ax.set_ylim(0, 400)\n",
    "    ax.set_facecolor('whitesmoke')\n",
    "    ax.grid(True, linestyle='--', color='lightgray')\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    x = np.arange(0, 401)\n",
    "    ax.plot(x, x, 'k-', lw=1.5, zorder=1)\n",
    "    ax.plot([0, 400], [70, 70], 'k--')\n",
    "    ax.plot([70, 70], [0, 400], 'k--')\n",
    "    ax.plot([0, 400], [180, 180], 'k--')\n",
    "    ax.plot([180, 180], [0, 400], 'k--')\n",
    "    \n",
    "    zone_counts = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'E': 0}\n",
    "    total_points = len(y_true)\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if (abs(true - pred) / true < 0.2) or (true < 70 and pred < 70):\n",
    "            zone_counts['A'] += 1\n",
    "        elif (true >= 70 and pred <= 50) or (true <= 70 and pred >= 180):\n",
    "            zone_counts['D'] += 1\n",
    "        elif (true > 180 and pred < 70) or (true < 70 and pred > 180):\n",
    "            zone_counts['E'] += 1\n",
    "        else:\n",
    "            zone_counts['B'] += 1\n",
    "            \n",
    "    print(\"\n--- Clarke Error Grid Analysis ---\")\n",
    "    if total_points > 0:\n",
    "        for z, c in zone_counts.items():\n",
    "            print(f\"  Zone {z}: {c}/{total_points} ({(c/total_points)*100:.2f}%)\")\n",
    "            \n",
    "    plt.savefig(os.path.join(plots_folder, f\"ClarkeGrid_{title.replace(' ', '_').replace(':', '')}.png\"), dpi=150)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def preprocess_and_feature_engineer(fnirs_path, cgm_path, cgm_column):\n",
    "    \"\"\"Loads, preprocesses, and engineers features from fNIRS and CGM data.\"\"\"\n",
    "    df_fnirs = pd.read_csv(fnirs_path)\n",
    "    df_fnirs.columns = df_fnirs.columns.str.strip()\n",
    "    df_cgm = pd.read_csv(cgm_path)\n",
    "    df_cgm.columns = df_cgm.columns.str.strip()\n",
    "    df_cgm['datetime'] = pd.to_datetime(df_cgm['Device Timestamp'], dayfirst=True)\n",
    "    df_cgm = df_cgm.sort_values(by='datetime').reset_index(drop=True)\n",
    "    first_cgm_time = df_cgm['datetime'].iloc[0]\n",
    "    df_cgm['Time_sec'] = (df_cgm['datetime'] - first_cgm_time).dt.total_seconds()\n",
    "    df_fnirs['glucose'] = np.interp(x=df_fnirs['Time'], xp=df_cgm['Time_sec'], fp=df_cgm[cgm_column])\n",
    "    \n",
    "    for s, d, ctype in USABLE_CHANNELS:\n",
    "        pmode, dval = ('LP', D_SHORT_CM) if ctype == 'short' else ('RP', D_LONG_CM)\n",
    "        cid, c740, c850 = f'S{s}_D{d}_{pmode}', f'S{s}_D{d}_740nm_{pmode}', f'S{s}_D{d}_850nm_{pmode}'\n",
    "        if c740 not in df_fnirs.columns or c850 not in df_fnirs.columns:\n",
    "            continue\n",
    "        od740 = -np.log10(np.maximum(df_fnirs[c740] / np.nanmean(df_fnirs[c740]), 1e-9))\n",
    "        od850 = -np.log10(np.maximum(df_fnirs[c850] / np.nanmean(df_fnirs[c850]), 1e-9))\n",
    "        hbo, hbr = (E_INV_MATRIX_uM @ np.vstack((od740 / (dval * DPF_WL1), od850 / (dval * DPF_WL2))))\n",
    "        df_fnirs[f'{cid}_dHbO_s'] = pd.Series(hbo).rolling(SMOOTHING_WINDOW, center=True, min_periods=1).mean()\n",
    "        df_fnirs[f'{cid}_dHbR_s'] = pd.Series(hbr).rolling(SMOOTHING_WINDOW, center=True, min_periods=1).mean()\n",
    "        \n",
    "    hb_cols = [c for c in df_fnirs.columns if '_dHb' in c and '_s' in c]\n",
    "    sr = 1 / df_fnirs['Time'].diff().mean()\n",
    "    samples_epoch = int(EPOCH_DURATION_S * sr)\n",
    "    step = int(samples_epoch * (1 - EPOCH_OVERLAP_RATIO))\n",
    "    epochs, labels = [], []\n",
    "    \n",
    "    for i in range(0, len(df_fnirs) - samples_epoch + 1, step):\n",
    "        epoch_df = df_fnirs.iloc[i:i + samples_epoch]\n",
    "        features = {}\n",
    "        for col in hb_cols:\n",
    "            s = epoch_df[col].dropna()\n",
    "            f = {'mean': s.mean(), 'std': s.std(), 'skew': s.skew(), 'kurtosis': s.kurtosis(), 'max_minus_min': s.max() - s.min()}\n",
    "            for k, v in f.items():\n",
    "                features[f'{col}_{k}'] = v\n",
    "        epochs.append(features)\n",
    "        labels.append(epoch_df['glucose'].mean())\n",
    "        \n",
    "    X = pd.DataFrame(epochs)\n",
    "    y = np.array(labels)\n",
    "    X.dropna(axis=1, how='all', inplace=True)\n",
    "    X.fillna(X.mean(), inplace=True)\n",
    "    return X, y\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name, experiment_name, plots_folder):\n",
    "    \"\"\"Calculates and prints performance metrics and generates plots.\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    print(f'\n--- Performance for {model_name} on {experiment_name} ---\n')\n",
    "    print(f'  RMSE: {rmse:.3f} mmol/L')\n",
    "    print(f'  MAE:  {mae:.3f} mmol/L')\n",
    "    print(f'  R²:   {r2:.3f}')\n",
    "\n",
    "    plot_title = f'{model_name} on {experiment_name}'\n",
    "    plot_clarke_error_grid(y_true, y_pred, plot_title, plots_folder)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Experiment Runners ---\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "def run_generalization_experiment(train_data, test_data, experiment_name, cv_strategy):\n",
    "    \"\"\"\n",
    "    Trains models on one session and tests on another.\n",
    "    \"\"\"\n",
    "    X_train, y_train = train_data\n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    print('=' * 70)\n",
    "    print(f'--- Running Generalization Experiment: {experiment_name} ---')\n",
    "    print(f'Training data shape: {X_train.shape}')\n",
    "    print(f'Test data shape:     {X_test.shape}')\n",
    "    print(f'CV Strategy: {CV_STRATEGY}')\n",
    "    print('=' * 70)\n",
    "\n",
    "    plots_folder = os.path.join(BASE_DIR, f'PLOTS_{experiment_name}')\n",
    "    os.makedirs(plots_folder, exist_ok=True)\n",
    "\n",
    "    for name, config in MODELS_AND_PARAMS.items():\n",
    "        print(f'\n--- Tuning and Training {name} ---')\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('selector', SelectKBest(f_regression, k=min(N_FEATURES_TO_SELECT, X_train.shape[1]))),\n",
    "            ('regressor', config['model'])\n",
    "        ])\n",
    "\n",
    "        grid_search = GridSearchCV(pipeline, config['params'], cv=cv_strategy, n_jobs=-1, scoring='r2')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        print(f'Best parameters for {name}: {grid_search.best_params_}')\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        evaluate_model(y_test, y_pred, name, experiment_name, plots_folder)\n",
    "\n",
    "def run_combined_holdout_experiment(s1_data, s2_data, cv_strategy):\n",
    "    \"\"\"\n",
    "    Trains on a combined dataset and tests on holdout sets from each session.\n",
    "    \"\"\"\n",
    "    X_s1, y_s1 = s1_data\n",
    "    X_s2, y_s2 = s2_data\n",
    "    \n",
    "    experiment_name = 'Combined_Holdout'\n",
    "    print('=' * 70)\n",
    "    print(f'--- Running {experiment_name} Experiment ---')\n",
    "    print(f'CV Strategy: {CV_STRATEGY}')\n",
    "    print('=' * 70)\n",
    "\n",
    "    plots_folder = os.path.join(BASE_DIR, f'PLOTS_{experiment_name}')\n",
    "    os.makedirs(plots_folder, exist_ok=True)\n",
    "\n",
    "    # Create chronological splits\n",
    "    X_s1_train, X_s1_test, y_s1_train, y_s1_test = train_test_split(X_s1, y_s1, test_size=1 - TRAIN_SPLIT_RATIO, shuffle=False)\n",
    "    X_s2_train, X_s2_test, y_s2_train, y_s2_test = train_test_split(X_s2, y_s2, test_size=1 - TRAIN_SPLIT_RATIO, shuffle=False)\n",
    "\n",
    "    # Combine training data\n",
    "    X_train_combined = pd.concat([X_s1_train, X_s2_train], ignore_index=True)\n",
    "    y_train_combined = np.concatenate([y_s1_train, y_s2_train])\n",
    "    \n",
    "    print(f'Combined training data shape: {X_train_combined.shape}')\n",
    "\n",
    "    for name, config in MODELS_AND_PARAMS.items():\n",
    "        print(f'\n--- Tuning and Training {name} on Combined Data ---')\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('selector', SelectKBest(f_regression, k=min(N_FEATURES_TO_SELECT, X_train_combined.shape[1]))),\n",
    "            ('regressor', config['model'])\n",
    "        ])\n",
    "\n",
    "        grid_search = GridSearchCV(pipeline, config['params'], cv=cv_strategy, n_jobs=-1, scoring='r2')\n",
    "        grid_search.fit(X_train_combined, y_train_combined)\n",
    "\n",
    "        print(f'Best parameters for {name}: {grid_search.best_params_}')\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Evaluate on Session 1 holdout\n",
    "        y_s1_pred = best_model.predict(X_s1_test)\n",
    "        evaluate_model(y_s1_test, y_s1_pred, name, 'Test on S1 Holdout', plots_folder)\n",
    "\n",
    "        # Evaluate on Session 2 holdout\n",
    "        y_s2_pred = best_model.predict(X_s2_test)\n",
    "        evaluate_model(y_s2_test, y_s2_pred, name, 'Test on S2 Holdout', plots_folder)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Main Execution ---\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('--- Starting Data Preprocessing ---')\n",
    "    s1_data = preprocess_and_feature_engineer(S1_FNIRS_PATH, S1_CGM_PATH, S1_CGM_COLUMN)\n",
    "    s2_data = preprocess_and_feature_engineer(S2_FNIRS_PATH, S2_CGM_PATH, S2_CGM_COLUMN)\n",
    "    print('--- Data Preprocessing Complete ---')\n",
    "\n",
    "    cv_strategy = get_cv_strategy(CV_STRATEGY, N_CV_SPLITS, CV_GAP_EPOCHS)\n",
    "\n",
    "    if RUN_GENERALIZATION_EXPERIMENTS:\n",
    "        run_generalization_experiment(train_data=s1_data, test_data=s2_data, experiment_name='Train_S1_Test_S2', cv_strategy=cv_strategy)\n",
    "        run_generalization_experiment(train_data=s2_data, test_data=s1_data, experiment_name='Train_S2_Test_S1', cv_strategy=cv_strategy)\n",
    "\n",
    "    if RUN_COMBINED_HOLDOUT_EXPERIMENT:\n",
    "        run_combined_holdout_experiment(s1_data=s1_data, s2_data=s2_data, cv_strategy=cv_strategy)\n",
    "\n",
    "    print('\nAll experiments complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}